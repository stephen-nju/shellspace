# # ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True
# # wait
# # ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True
# # wait
# ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32
# wait
# ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32
# wait
# ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32
# wait
# ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32
# wait
# ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initTrue_pissa_iter16_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init True --pissa_iter 16
# wait
# ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16
# wait
# ./mllmtrain.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convertFalse_pissa_initTrue_pissa_iter16_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convert False --pissa_init True --pissa_iter 16
# wait
# ./mllmtrain.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convertFalse_pissa_initTrue_pissa_iter16_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convert False --pissa_init True --pissa_iter 16
# wait
# ./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True
# wait
# ./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq1024_lr5e-06 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 5e-06 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True
# wait
# ./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_xxy_schedule_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True
# wait
# ./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_xxy_schedule_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq1024_lr0.0001 --batch_size 4 --cutoff_len 1024 --dataset xxy_schedule --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True
# wait
##########################################################################
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rym_ner --dataset rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_common_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name common_rym_ner --dataset common_rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_yjzl_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name yjzl --dataset yjzl_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_thzy_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name thzy --dataset thzy_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_rccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rccq --dataset rccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_gjccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name gjccq --dataset gjccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_tzjx_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name tzjx --dataset tzjx_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rym_ner --dataset rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_common_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name common_rym_ner --dataset common_rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_yjzl_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name yjzl --dataset yjzl_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_thzy_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name thzy --dataset thzy_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_rccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rccq --dataset rccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_gjccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name gjccq --dataset gjccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_tzjx_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionTrue_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower True --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name tzjx --dataset tzjx_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rym_ner --dataset rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_common_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name common_rym_ner --dataset common_rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_yjzl_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name yjzl --dataset yjzl_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_thzy_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name thzy --dataset thzy_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
# ./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_rccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node11 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rccq --dataset rccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
# wait
./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_gjccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name gjccq --dataset gjccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-2B-Instruct_tzjx_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name tzjx --dataset tzjx_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rym_ner --dataset rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_common_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name common_rym_ner --dataset common_rym_ner_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_yjzl_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name yjzl --dataset yjzl_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_thzy_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name thzy --dataset thzy_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_rccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name rccq --dataset rccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_gjccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name gjccq --dataset gjccq_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
./mllmtrain.sh --name 'Qwen2-VL-7B-Instruct_tzjx_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_freeze_visionFalse_maxseq2048_lr0.0001' --batch_size 4 --cutoff_len 2048 --do_train True --epochs 3 --freeze_vision_tower False --gradient_accumulation_steps 1 --include node3 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --task_name tzjx --dataset tzjx_train --finetuning_type lora --lora_rank 32 --lora_target 'all'
wait
